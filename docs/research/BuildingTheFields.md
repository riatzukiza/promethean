# Building the “Eidolon Fields” Brain Architecture

## Vision: A “Franken-Brain” with Multiple Cognitive Layers

**Goal:** We aim to create an AI model that combines the strengths and knowledge of multiple language models into a single system—**a “franken-brain”** that no one can ignore, yet is accessible to everyone. Instead of training one model in isolation, we will fuse knowledge from many models and structure it in a novel architecture inspired by human cognition. This approach is motivated by recent techniques for merging or “fusing” LLMs (large language models) to achieve capabilities beyond any single source model. By leveraging open models (like LLaMA and others) and **PyTorch + HuggingFace** tooling, we can integrate diverse expertise into one network and fine-tune it for broad, powerful performance.

**Concept:** Our model is built around the idea of **eight cognitive layers**, each corresponding to a fundamental aspect of human thought (drawing on the eight-circuit model from _Prometheus Rising_). We call these layers **“Eidolon Fields.”** In essence, each layer is a knowledge repository and processing unit specializing in one dimension of cognition, but all layers are interconnected like a cohesive brain. The name “Eidolon” (meaning an idealized form or ghostly double) reflects that each field is a projection of knowledge along a certain mindset or perspective.

## The Eidolon Fields Architecture (8×8×8 Structure)

**Eight Layers for Eight Aspects:** We designate one layer for each of the eight circuits of consciousness proposed by Timothy Leary and Robert Anton Wilson. In brief, these are:

1. **Bio-Survival (Layer 1):** Basic safety, survival instincts, physical needs.
    
2. **Emotional/Territorial (Layer 2):** Power, dominance, submission, emotional responses.
    
3. **Semantic/Symbolic (Layer 3):** Rational mind, language, logic, time-binding knowledge (traditional intellect).
    
4. **Socio-Sexual (Layer 4):** Social roles, moral norms, interpersonal bonding (including sexual behavior and ethics).
    
5. **Holistic Neurosomatic (Layer 5):** Body-mind connection, sensory bliss, creativity, and wellness (integrating emotions with bodily awareness).
    
6. **Collective Neurogenetic (Layer 6):** Ancestral memory, archetypes, collective unconscious, evolutionary instincts.
    
7. **Meta-Programming (Layer 7):** Self-awareness of mind, ability to reprogram one’s own thought processes (introspection, creativity in thinking).
    
8. **Non-Local Quantum (Layer 8):** Mystical or transcendental thinking, feeling of oneness, big-picture cosmic perspective.
    

Each “circuit” or layer thus loosely represents a **functional domain** of cognition, from the very concrete (survival reflexes) to the very abstract (cosmic consciousness). By structuring the AI’s knowledge into these layers, we hope to emulate something akin to human-like multi-faceted thinking, where different mental faculties contribute to an answer.

**8-Dimensional Fields:** Within each of the eight layers, we create an **eight-dimensional field of vectors**. This means we imagine an 8D space (with 8 independent axes) where each point in that space holds an 8-dimensional feature vector. In other words, each layer is like a high-dimensional grid or lattice of learned vectors. We plan to discretize each axis into about **4–8 segments** (positions) along that dimension. This yields a finite grid of points – for example, if we use 6 segments per axis on average, that’s 6^8 ≈ 1.68 million points per layer. Each point stores an 8-value embedding vector representing some micro-concept or state. The idea is that each axis of this field corresponds to a **meaningful factor or sub-aspect** of that layer’s cognition.

- _Example:_ In the Bio-Survival layer, the 8 axes might represent basic sensory or need parameters (e.g. hunger–satiety, danger–safety, pain–pleasure, etc.). A coordinate in this 8D space (like a combination of “hungry, safe, no-pain, …”) maps to an 8D vector capturing what the “mental state” or knowledge is in that situation. Likewise, in the Emotional layer, axes could represent emotional valences (happy–sad, confident–fearful, etc.), and each coordinate corresponds to a nuanced emotional context vector.
    

**Interaction by Distance:** The fields are not isolated; they influence each other. We impose a notion of _distance_ on the combined space of all eight fields. If two points are close in their 8-dimensional coordinates (even if they belong to different layers), they can interact and share information. Interactions within each field are **biased toward that field’s primary dimension**. This means, for example, in Layer 1 (survival), differences along the “survival” axis matter a lot for connectivity (neighbors along that axis strongly influence each other), whereas differences along other axes (perhaps representing emotional or social context) have a smaller effect. Each layer strongly emphasizes its own key dimension (the one most related to its cognitive circuit) in how information flows locally, yet **all 8 dimensions exist in all layers**, providing a common representational space. Essentially, we embed all layers in a unified 8-dimensional conceptual space, where each layer is a slice that leans heavily into one axis.

- **Within-Layer Interaction:** We can imagine each layer as an 8D cellular automaton or grid where each cell (point) interacts mainly with its near neighbors. “Near” is defined by the grid distance—two points differing only slightly in coordinates (especially along the layer’s main axis) will exchange or blend their vectors. This lets each field organize knowledge smoothly within its domain (e.g. similar survival scenarios have similar vectors, facilitating interpolation).
    
- **Cross-Layer Interaction:** Because all layers share the same coordinate system (the axes have consistent meanings across layers), a point in one layer can find “nearest neighbor” points in other layers. We will allow a degree of influence where Layer 1’s point at coordinate (x1,…,x8) can receive input from Layer 2’s point at roughly (x1,…,x8) if those coordinates are close. The farther apart two points are in this 8D concept space, the less they affect each other. In effect, **each piece of knowledge in one layer may activate related knowledge in other layers if their conceptual content is similar**. The strength of this influence might be tuned by a distance threshold or a kernel function. This design means that while each layer specializes (e.g. survival-oriented interpretations vs. emotional interpretations of a situation), they all contribute to a holistic understanding when the model processes an input.
    

**Dynamic Representation:** How will the model actually use these fields? The workflow might be as follows:

- An input (say, a user question or prompt) is analyzed and mapped to positions in each of the eight fields. For instance, we might design a small module or use keywords to estimate where the prompt lies along each cognitive axis. If the question is _“Should I be afraid of investing money in stocks?”_, our system might map this to something like: high on survival anxiety axis (fear of loss), moderate on emotional (anxiety), involving social/semantic aspects (it’s a rational financial question with emotional overtones). We would thus query points in Layer1 (fearful survival scenario), Layer2 (anxious emotional state), Layer3 (financial knowledge semantics), Layer4 (maybe societal norms about money), etc.
    
- We retrieve or interpolate the vectors at those coordinates from each layer’s field. This gives us eight 8-dimensional vectors (one per layer) representing the input through different “lenses.”
    
- These vectors can then be combined or attended to by a subsequent neural network module (e.g. concatenated or summed into a single context vector, or fed as keys/values to an attention mechanism) which will generate the output (answer).
    
- The model might iterate, refining each layer’s coordinates based on feedback from the others (simulating a thought process that bounces between emotional insight, rational analysis, etc., until it stabilizes on an answer). This could be implemented with a recurrent loop or an iterative message passing over the 8 fields, halting after a few steps to produce the final answer.
    

**Note:** The 8D fields likely contain _learned_ values (parameters). We might initialize them randomly or even seed them with knowledge embeddings. During training, these field vectors will adjust so that the model can accurately use them to answer questions. Effectively, the fields serve as a **distributed memory** for each cognitive aspect, and training teaches the model how to access and update this memory.

## Merging Knowledge from Multiple Models (“Franken-Brain” Assembly)

To populate and train our Eidolon Fields model, we will leverage **existing models as knowledge sources**. The idea is to **sample or distill knowledge from each model** (each model being a specialist or having unique strengths) and transfer it into the corresponding part of our system. Recent research supports the feasibility of merging multiple models’ strengths into one, either by weight merging or by knowledge distillation/fusion. We will employ several strategies:

- **Model Selection:** We’ll choose a set of open-source models that collectively cover a wide range of capabilities. For example:
    
    - A strong general knowledge model (e.g. LLaMA-2 13B chat-tuned) as a baseline for semantics (Layer 3).
        
    - A model known for reasoning or code (to contribute logical problem-solving ability, possibly Layer 3 and 7).
        
    - A model fine-tuned on emotional conversational data (to inform Layer 2).
        
    - Perhaps a model specialized in creative writing or psychedelic/philosophical text (to inform Layers 5 and 8).
        
    - Models with domain expertise (e.g. a medical or financial specialist model) if needed for certain circuits.
        
    
    The exact “roster” can be adjusted, but the key is diversity: each source model offers something unique.
    
- **Ensemble Knowledge Distillation:** Rather than deploying all these models at runtime (which would be impractical), we perform a form of **knowledge fusion** via distillation. In knowledge fusion, we use the source models to generate training data or probability distributions that teach our target model. For a given input, each source model provides its perspective (its next-word probabilities or its answer). We can combine these outputs (e.g. by weighted averaging of their probability distributions over tokens) to create a “fused” target distribution. Our Eidolon model is then **trained (continual fine-tuning)** to mimic this fused distribution, thus absorbing the collective knowledge. This approach was demonstrated in the FuseLLM method, which aligned token probabilities from models like LLaMA-2, MPT, and OpenLLaMA, and achieved a single student model that outperformed any individual source on various tasks. By externalizing each model’s strengths in this way, we aim to elevate the Eidolon model’s capabilities beyond any single teacher.
    
- **Layer-Specific Data:** We can further tailor the distillation or fine-tuning to our eight layers:
    
    - For each cognitive layer, we will prepare prompts or tasks that emphasize that aspect. For instance, to train Layer 1 (survival instincts), we might include many QA pairs about dangers, security, physical well-being, etc., and rely on a teacher model (or a combination) to generate “survival-oriented” answers. To train Layer 2 (emotional), we include emotionally charged dialogues or psychological advice questions. Layer 4 (social/moral) might see a lot of ethical dilemma questions or social interaction simulations, and so on.
        
    - By doing this, we effectively _focus_ each part of the Eidolon Fields on its domain during training. The model’s parameters within Layer 4, for example, will learn from situations requiring moral judgments or social understanding, ideally imprinting those skills there.
        
    - We will still allow cross-layer influence during training. If a prompt spans multiple aspects (as most real queries do), it will activate multiple layers and the gradient will propagate to all relevant parts. This trains the model not just to handle pure single-circuit questions, but also integrated ones.
        
- **Weight Merging for Identical Architectures:** In cases where two source models share the same architecture (e.g. LLaMA 7B base model fine-tuned on different tasks), we can attempt direct **parameter merging**. Techniques like model souping (averaging weights) or more advanced merges (e.g. spherical interpolation or task vector addition) can combine fine-tuned models without original training data. For instance, if we have a base LLaMA and two LoRA adapters (low-rank fine-tunes) – one for math reasoning, one for storytelling – we can merge these adapters into one model. Tools like **LoRAX** provide methods to linearly combine multiple LoRA adapters or use task arithmetic (subtracting the base, adding deltas) to incorporate multiple skills. This merging is done carefully to avoid destructive interference between skills:
    
    - _Linear weight averaging:_ Simply average the weights or LoRA deltas from different models. This assumes the fine-tunings are compatible; it often works best when they started from the same base and are fine-tuned on complementary tasks.
        
    - _TIES merging:_ A more sophisticated approach where we treat each fine-tune as a “task vector” (difference from base) and combine them, possibly sparsifying weights and taking a consensus of signs to reduce conflicts. This can retain multiple skills more robustly, even if the tasks have some conflicting updates.
        
    - _Iterative or evolutionary merging:_ If many models are involved, iterative merging or using an evolutionary search to choose layers from different models (sometimes called “Franken-merging”) is possible. For example, one could literally **stack** layers from different models – e.g., take lower layers from a model good at language understanding and upper layers from a model good at common-sense reasoning. This was used in some experiments (creating hybrid models like “Goliath” by concatenating layers from different sources). In our context, however, we’re designing a new architecture from scratch (the Eidolon fields), so layer stacking from existing models might not directly apply. Instead, we focus on integrating at the **knowledge level** rather than copy-pasting network sections.
        
- **Evaluation and Iteration:** As we merge knowledge, we will evaluate the combined model on a range of tasks (reasoning puzzles, emotional conversations, factual Q&A, etc.). We expect to iterate: if we find, for example, that the model is weak in “Circuit 5 – neurosomatic creativity” (perhaps it gives dull descriptive answers lacking sensory detail), we can fine-tune that aspect more, possibly by distilling from a highly creative model or adding relevant training data. The architecture’s modular nature means we could even fine-tune one layer at a time on targeted data (making sure not to wreck overall performance by testing after each such tune). Because each layer has some autonomy, a focused adjustment in one layer (with others frozen) could be an efficient way to inject new knowledge or correct deficiencies in that domain.
    

## Implementation Plan and Tools

We will use **PyTorch** and the HuggingFace **Transformers** library to implement this system. Although our architecture is non-standard, we can still use these frameworks to our advantage for things like loading pre-trained models, utilizing GPU acceleration, and managing training loops.

**1. Define the Model Architecture:** We will create a custom `nn.Module` for the Eidolon Fields. A simple implementation is to represent each layer’s 8D field as a multi-dimensional tensor of parameters. For example, `layer1_field` could be a tensor of shape `[d1, d2, ..., d8, 8]` (where each d_i is ~4–8). We might start smaller (e.g. 4 segments per dimension) to get a feel for training. This tensor will be learnable parameters. We might also have parameters for combining layers – for instance, weights that determine how to mix the eight layer vectors into the final output, or an attention mechanism that attends over layers.

**2. Input Encoding:** We need to map text input into our 8-dimensional coordinate for each layer. Initially, we can use a **pre-trained encoder** (like a smaller language model or just keyword matching) to estimate coordinates:

- We could hand-craft this mapping for a prototype (e.g., detect certain words: “danger, safe, food” raise the survival dimension; emotional tone words affect the emotional dimension; etc.). But a learned approach will be more robust.
    
- A practical solution is to use an existing transformer model’s embedding as input and train a small linear or MLP head that projects that embedding to an 8-dimensional vector (which we then discretize or use to interpolate in the grid). Essentially, _we will have a front-end that reads the prompt and outputs eight values (one per circuit axis)_ reflecting how much the prompt engages each cognitive dimension.
    
- We can incorporate this into training: the front-end (maybe a BERT or a distilled version of LLaMA encoder) and the Eidolon Fields can be trained together end-to-end on QA tasks so that the front-end learns to put the “address” in the right spot in each field that leads to a correct answer.
    

**3. Output Decoding:** The output could be generated token-by-token like a language model, or we could simplify by having the model generate an embedding that is fed into a pre-trained decoder. A straightforward approach:

- Concatenate or sum the eight vectors retrieved from each layer (after the input mapping). This gives a single 8-dimensional (or 64-dimensional if concatenated) representation. Then pass this through a decoder network (for example, a stack of Transformer decoder blocks or even directly into the embedding layer of a language model) to produce text.
    
- Another approach is to treat each layer as generating a “comment” or contribution to the answer and then combine them. However, merging at the embedding level is easier initially.
    
- We might use a small GPT-2 style generator that conditions on the combined vector and generates the answer sequence. This generator would be trained as part of the overall model (the loss flows back into the Eidolon fields and input encoder too).
    

**4. Fine-Tuning Procedure:** We will perform _multi-teacher knowledge distillation_ as discussed:

- Prepare a large set of prompts that cover a wide variety of topics and aspects. For each prompt, run each teacher model to get its answer (or its probability distribution per token if feasible).
    
- Construct a target output for training. If we have full model answers, we can train the Eidolon model to match the _content_ of the best answers (using a standard cross-entropy loss on the answer text). If we have token distributions, we can train by minimizing divergence between Eidolon’s next-token probabilities and an averaged distribution of teachers.
    
- Use the HuggingFace Trainer or a custom training loop in PyTorch to train the Eidolon model on this synthetic Q&A dataset. We will likely use a two-stage training: first, a phase where we freeze the front-end and decoder and just tune the Eidolon field parameters to quickly imprint knowledge, then a phase to fine-tune everything jointly for coherence.
    
- We must monitor training for signs of any one teacher dominating or knowledge conflicts. It might be useful to introduce loss weighting so that each layer’s relevant data is emphasized (e.g., ensure we include enough prompts that exercise each circuit to train those layers well).
    

**5. Iterative Refinement & Feedback:** After an initial training, we’ll test the model on interactive prompts. Because of the architecture’s transparency, we can attempt to interpret what it’s doing:

- We can inspect which coordinates were activated in each layer for a given question. This will tell us, for example, if a certain question fails, was it because one layer (say the rational semantic layer) pulled the answer in a wrong direction? We might see that, e.g., Layer 2 (emotional) wasn’t engaged when it should have been, leading to a too-cold response. We can then adjust the input mapping or add training examples to fix that.
    
- We can also use **interpretability tools** at the neural level. Since we plan to integrate multiple models’ knowledge, debugging is key. Tools like **TransformerLens** or the **EleutherAI “knowledge neurons” technique** can help us locate where specific facts or phrases are stored in the network. In fact, research has shown that certain _neurons in a transformer correspond to specific factual associations_, and one can identify them by analyzing activations for prompts containing that fact. We can apply this to our model to ensure critical knowledge (like factual correctness) is properly stored and to diagnose any hallucinations.
    
- If the model outputs an incorrect fact or an unwanted phrase, we can try **direct model editing** methods (instead of retraining from scratch). For example, the ROME technique (Rank-One Model Editing) finds a small weight update in a mid-layer feedforward that can change a specific known fact in the model. Similarly, the knowledge neurons approach allows us to pinpoint neurons responsible for a given fact and modify or suppress them. Using these, we can perform “surgery” on the Eidolon model’s memory: e.g., ensure it unlearns a training hallucination or updates to a recent factual change, without full retraining. This fine-grained control will be part of our toolkit as we polish the model.
    

**6. Deployment via Ollama:** Once we are satisfied with the model’s performance, we will package it for **Ollama**. Ollama is a user-friendly runtime for local LLMs, and publishing the model there will put it “into everyone’s hands.” This involves converting our PyTorch model to the format Ollama expects (likely a GGML or similar optimized format for local inference). We will also prepare documentation of our findings (what worked best in merging, how each layer contributes, etc.) to accompany the model. The end result will be an easy-to-run local AI that embodies our eight-layer franken-brain. Users would not need to know the complex inner workings – they would simply prompt it like any chat model – but under the hood, it’s consulting multiple cognitive “fields” before speaking, which we anticipate yields more balanced and context-aware responses.

## Interpretability and Future Improvements

One exciting aspect of the Eidolon Fields design is that it’s inherently more interpretable than a monolithic model. Each layer has a conceptual meaning, so we can introspect the model’s reasoning path. We plan to log or visualize which layers/coordinates are most active for a given query. For example, if a question triggers mainly Layer 3 (semantic) and Layer 7 (metaprogramming), we know the model is handling it in a dry, analytical way; if we see Layer 2 and Layer 4 lighting up, the answer may involve empathy and social considerations. This transparency not only builds trust but also allows fine-targeted moderation (if one layer is producing unwanted bias, we can focus our fixes there).

To facilitate this, we might implement **highlighting of network activations** for a given phrase or output. As the user alluded, there are tools to input a phrase and see which part of the network was responsible. We can adapt the knowledge neuron finding techniques for our architecture to highlight, say, _which layer’s neurons_ strongly contributed to a specific word in the output. If something looks off, we then “give it another phrase and tweak it” – essentially experiment by adjusting either the prompt or the internal weights and observing changes. These controlled interventions are possible thanks to published methods that show positive results in editing models without retraining.

**Longer-term improvements:** After initial deployment, we anticipate iterating on the model:

- **Scaling Up:** Possibly increase the resolution of each field (more segments per dimension) or even the dimensionality of vectors stored (maybe use 16d vectors at each point instead of 8d for more richness). This would increase parameters and capacity.
    
- **Adaptive Coordinates:** Right now, we consider discrete segments, but we could allow continuous coordinates and use interpolation (e.g., trilinear or rather “octa-linear” interpolation in 8D) to get smoother representation. A small neural network could learn to interpolate between the nearest grid cells. This would make the fields more like continuous latent spaces rather than fixed slots.
    
- **Layer Communication Mechanism:** We might upgrade the simple distance-based influence to a learned attention mechanism between layers. For instance, each layer could have an attention head that can attend to the others’ outputs, learning which layers to consult for a given context. This would be akin to how different expert networks in a mixture-of-experts model sometimes exchange information. Ensuring this doesn’t collapse into one layer dominating will be a design consideration.
    
- **Reinforcement Learning from Feedback:** We can use RLHF (Reinforcement Learning from Human Feedback) to fine-tune the model’s outputs to be more helpful or aligned. Interestingly, RLHF might differentially affect the layers – e.g., human feedback might prefer answers that have a certain balance of rational vs. emotional content, nudging those layers accordingly. Monitoring layer activations during RLHF training could give insight into _how_ the model is altering its internal reasoning to please users.
    

In summary, we are embarking on an ambitious build: an AI brain that explicitly separates and then reunifies different modes of thinking. By **merging the knowledge of many models and structuring it in eight “Eidolon” fields**, we hope to achieve an AI that is exceptionally knowledgeable, versatile, and tuned into the various facets of human-like cognition. The use of cutting-edge model fusion techniques will ensure it has a broad knowledge base, and the unique architecture will make it _feel_ more intuitive and context-aware in its responses. We have outlined the plan grounded in current research and tools, and with careful implementation and iteration, we’ll create something truly powerful and shareable.

Ultimately, this franken-brain won’t be a monstrosity but rather a **collective intelligence** in one body – the sum greater than its parts – and we’re excited to put it into everyone’s hands.

**Sources:**

- R.A. Wilson, _Prometheus Rising_ – Eight-circuit consciousness model
    
- Knowledge Fusion of LLMs (FuseLLM, 2024) – merging multiple models into one
    
- LoRA and Model Merging Techniques – combining fine-tuned adapters for multi-skill models
    
- Dai et al. (2021), _Knowledge Neurons in Pretrained Transformers_ – identifying neurons that store specific facts and editing them
    
- Meng et al. (2022), _Locating and Editing Factual Associations in GPT (ROME)_ – precise weight updates to modify factual knowledge
\#hashtags: #research #eidolon #promethean
